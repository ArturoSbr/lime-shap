# Interpreting Machine Learning models using LIME andÂ SHAP

When someone is allowed to act autonomously, it is important to understand how and why they make decisions. Why does a judge determine that a party is guilty? Why does a doctor prescribe one treatment instead of another? Why does a screenwriter decide to kill a character at the end of a season?

It is not hard to imagine that one day lawsuits, medicine, screenwriting and many other tasks will be carried out by Artificial Intelligence (AI) models. Just as before, we would like to know how and why a decision was made. Unlike humans however, we cannot interview an AI model to enquire about its decision making process, which is why methods to explain models' predictions are important.

This repository is a companion to an upcoming Svitla article (link will be available soon) that covers LIME and SHAP methodologies to explain the predictions made by *any* kind of artificial intelligence model.
